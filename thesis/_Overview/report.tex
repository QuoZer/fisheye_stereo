\include{settings}

\begin{document} % начало документа
\raggedbottom
%\include{titlepage}


% Содержание
%\tableofcontents
%\newpage
\title{Обзор алгоритмов стереозрения и их применимости для получения результатов в реальном времени}
\author{Пантелеев М.}
\institute{Санкт-Петербургский Политехнический Университет Петра Великого
\email{panteleev.md@edu.spbstu.ru}}
\maketitle

\begin{abstract}
	% TODO: дополнить  
	В этой статье приводится обзор основных алгоритмов, связанных с получением глубины по снимкам одной сцены с нескольких камер. Улучшение существующих и
	создание новых методов происходит постоянно, так как проблема точного и быстрого стерео до сих пор не решена. Разные подходы склоняются либо в сторону высокой, 
	производительности, либо в сторону повышенной точности. Алгоритмы, способные выполнять сопоставление в реальном времени представляют особый интерес для 
	исследователей, поэтому выбранные методы рассматриваются также на возможность такого применения.

\end{abstract}

\section{Введение}
Получение трёхмерной структуры пространства по стереоснимкам - это задача, подходы к которой начали искать очень давно. И хотя 
множество решений было найдено, исследователи продолжают работу над более эффективными алгоритмами. В силу дешевизны (относительно других сенсоров,
позволяющих получить объёмную информацию о пространстве) и универсальности камер, на подобные методики есть большой спрос со стороны 
таких областей как автономный транспорт и мобильная робототехника. 					% TODO: уточнить
Эти области весьма требовательны к точности результатов и скорости работы алгоритмов, так как устройства должны принимать оперативные решения на основе 
данных о внешнем мире, а мощности бортовых вычислителей весьма ограничены. Именно эти качества находятся в фокусе современных исследований, поэтому в этой 
работе также будет уделено внимание возможности того или иного алгоритма работать в реальном времени. Эта возможность характеризуется частотой, с которой 
алгоритм просчитывает карты глубины. Очевидно, что фактическое значение частоты кадров сильно зависит от используемого вычислителя \cite{archs}, разрешения камер и диапазона 
глубины, который способен определить алгоритм, поэтому для теоретического сравнения больше подходит вычислительная сложность алгоритма. К сожалению, не 
все авторы приводят оценку вычислительной сложности собственных алгоритмов, в этих случаях указана реальная производительность, заявленная авторами или 
одним из популярных бенчмарков. 

При этом в стереозрении есть ряд проблем и ограничений, которые осложняют получение надёжной и полной информации по снимкам. Часть этих проблем 
связана с недостатками самого способа получения изображений (заслонение), другая часть - с особенностями отображения некоторых поверхностей на 
двумерных снимках (наклонные плоскости или слаботекстурированные области). Снижение влияния этих 
особенностей на результат путём создания новых алгоритмов или модификации существующих является второй важной целью исследователей. 

Остальная часть статьи организована следующим образом: в продолжении этой секции рассмотрен базовый принцип стереозрения, секция \ref{matching} описывает 
самые популярные подходы к построению карт расхождения и их применимость для получения результатов в реальном времени, в секции \ref{conclusion} представлены выводы. 

\subsection{Принцип стереозрения}
Несмотря на существование разных алгоритмов стереозрения, все они реализуют общий принцип. Задача стереозрения 
 состоит в использовании двух или более камер для получения данных о дальности до объектов в кадре. 

Как правило, система стереозрения состоит из двух камер, наблюдающих сцену с разных точек, как изображено на рисунке \ref{pic:epipol} \cite{Hartley2004}. 
Фундаментальная основа принципа заключается в предположении, что каждой точке в пространстве соответствует уникальная пара пикселей на снимках с двух камер.  

При этом к камерам предъявляются некоторые требования \cite{rusoverview}:   % не уверен, что это надо цитировать
\begin{itemize}
	\item Камеры откалиброваны. Это значит, что известны внутренние (оптические) и внешние (расположение камер в пространстве) параметры камер. 
	\item Ректификация. Подразумевает выравнивание изображения с обеих камер по строкам.  % Мб подробнее расписать  
	\item Ламбертовость поверхностей. Означает независимость освещения поверхности от угла зрения. 
\end{itemize}

\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.5]{pics/epipolar geometry.png}
		\caption{Эпиполярная геометрия} 
		\label{pic:epipol} % название для ссылок внутри кода
	\end{center}
\end{figure}

Таким образом, соблюдение указанных выше требований позволяет использовать следующий геометрический принцип. При наличии двух камеры, как изображено 
на рисунке \ref{pic:epipol}, где $C$ — центр первой камеры, $C'$ — центр второй камеры, точка пространства $X$  
проецируется в $x$ на плоскость изображения левой камеры и в $x'$ на плоскость изображения правой камеры. Прообразом точки $x$ на изображении левой 
камеры является луч $xX$. Этот луч проецируется на плоскость второй камеры в прямую $l'$, называемую эпиполярной линией. Образ точки $X$ на плоскости 
изображения второй камеры обязательно лежит на эпиполярной линии $l'$.

В результате каждой точке $x$ на изображении левой камеры соответствует эпиполярная линия $l'$ на изображении правой камеры. При этом соответствие для $x$ на 
изображении правой камеры может лежать только на соответствующей эпиполярной линии. Аналогично, каждой точке $x'$ на правом изображении соответствует 
эпиполярная линия $l$ на левом.

Далее с помощью точек $x$ и $x'$ возможно посчитать смещения каждого пикселя одного изображения относительно другого, что даёт карту смещений (disparity map). 
Очевидно, что смещения будут подсчитаны только для точек, видимых обеими камерами. Карта смещений же приводится далее либо к облаку точек, либо к карте глубины. 
Пример такой карты представлен на рисунке \ref{pic:depth} \cite{lipson2021raft}. 

\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.7]{pics/exmpl.jpg}
		\caption{Примеры результата работы} 
		\label{pic:depth} % название для ссылок внутри кода
	\end{center}
\end{figure}

На практике работу большинства алгоритмов можно разделить на 3 этапа: получение изображений, поиск соответствий и восстановление информации о глубине. Это позволяет
организовать классификацию алгоритмов на основе подходов к каждому из этих этапов. В этой статье будут рассмотрены в основном лишь методы поиска соответствий на изображениях. 

Восстановление заключается в расчёте глубины по известным внешним параметрам камер. Глубина каждой точки $X$, воспринимаемой двумя камерами 
с оптическими центрами $C$ и $C'$, определяется проекциями $x$, $x'$ этой точки на каждый кадр. Тогда значение глубины можно найти по уравнению \ref{eq:Z}, указанному ниже. 
\begin{equation}	
	Z = f{T \over d},
	\label{eq:Z}
\end{equation}
где T - расстояние между $C$ и $C'$;
    d - смещение,  {d = x - x'};
	f - фокусное расстояние камеры.

Получение изображений обычно осуществляется с использованием двух синхронизированных камер. Однако есть работы \cite{singlecamrev,singlecam}, которые 
решают эту задачу с использованием лишь одной камеры совместно с системой линз и зеркал, но принцип их функционирования по своей сути 
 симулирует двухкамерную реализацию. 
 
 Другой вектор работ направлен на получение информации о глубине по снимках, сделанным с помощью камер со сверхшироким 
 углом обзора, так как они часто встречаются в мобильных роботах \cite{omnifisheye,roxas2019realtime}, но существующие работы пока не могут обеспечить результаты, 
 сравнимые с традиционными камерами без существенных вычислительных затрат. 

\section{Поиск соответствий}
\label{matching}
Как было ранее сказано, поиск соответствий можно выполнять широким набором методов. Цель каждого метода - постараться найти для каждого пикселя одной картинки 
соответствующий ему пиксель на второй. Достичь этого можно двумя подходами, в зависимости от накладываемых на зону поиска соответствий ограничений.  При подсчёте 
расхождений лишь в небольшой окрестности (окне) вокруг интересующего нас пикселя мы говорим о локальных методах - эти методы  используют небольшое количество 
информации и относительно быстродействены. Их принято делить на три категории: сопоставление блоков, градиентные методы и 
сопоставление признаков. 		% TODO: я не особо следую этому, мб переписать
Глобальные методы же обрабатывают целый ряд пикселей или всё изображение целиком. Они не так чувствительны к локальным дефектам, мешающим процессу поиска соответствий 
(например, заслонение), но при этом имеют куда большую вычислительную сложность. Одними из самых популярных подходов в этой группе методов являются динамическое программирование \cite{dynamic_prog},
и алгоритм разреза графа. 

\subsection{Сопоставление блоков}
\label{BM}
Block Matching - это локальный метод, который заключается в оценке расхождения в точке на одной картинке с помощью сравнения небольшой области вокруг этой точки с такими же областями 
на другой \cite{}. Благодаря выпрямлению (ректификации) поиск соответствующей области на другой картинке ограничивается одним измерением. По этому измерению (как правило, строке) 
считается одна из доступных метрик, и регион с наименьшим её значением считается искомым. Существует целый набор метрик, часто используемых в этом методе. 

Для отдельных пар пикселей можно рассчитывать абсолютную разность (AD) или квадратичную разность (SD), выраженную через интенсивности:
\begin{equation}
	AD(x, y, d) = |I_l(x,y) - I_r(x+d, y)|,			
	\label{eq:AD}
\end{equation}
где $(x, y)$ - координаты первого пикселя; $d$ - сдвиг по оси x между двумя пикселями (смещение); $I$ - интенсивность данного пикселя. Обычно $I_l$ обозначают опорное изображение, а $I_r$ - 
целевое. Абсолютная разность - простейшая метрика, за счёт чего до сих пор используется в многих алгоритмах, от которых требуется производительность в реальном времени. 

Для квадратичной разности: 
\begin{equation}
	SD(x, y, d) = |I_l(x,y) - I_r(x+d, y)|^2.		
	\label{eq:SD}
\end{equation}

На основании этих метрик были выведены более точные и сложные, такие как сумма квадратичных разностей (SSD) и сумма абсолютных разностей (SAD). Они применяют описанный выше принцип для целого 
окна пикселей вокруг исследуемого.

Также применяется нормированная кросс-корреляция (NCC) - стандартный статистический метод для поиска соответствий с шаблоном, который записывается для случая поиска соответствий пикселей 
согласно уравнению \ref{equ:NCC}.

\begin{equation}
	NCC(x, y) = \frac{ \sum_{x, y}^{} (I_l(x, y) - \overline{I_l} )^2 * ( I_r(x + d, y) - \overline{I_r} )^2   }{ \sqrt{ \sum_{x, y}^{} (I_l(x, y) - \overline{I_l} )^2 * ( I_r(x + d, y) - \overline{I_r} )^2 }  }, 
	\label{equ:NCC}
\end{equation}

где $\overline{I_l}$ и $\overline{I_r}$ - средние интенсивности соответствующих изображений. Эта метрика устойчива к перепадам яркости и контрастности изображения благодаря нормализации \cite{ncceval}, 
но требует выполнения значительно большего числа арифметических операций для своего подсчёта. 

Таким образом, почти любой алгоритм сопоставления блоков позволяет получать относительно быстрые результаты сам по себе, но точность остаётся маленькой и чувствительной к различным условиям освещения. Поэтому многие 
работы в последние годы пытались сгладить недостатки этого метода, используя вместе с тем возможность независимого  подсчёта для параллелизации.    
Например, эти метрики позволяют повышать качество итоговой карты смещений за счёт варьирования размера окна и дополнительных проходов по изображениям \cite{twosizewindow}. 
Шан и другие \cite{SAD_FPGA} реализовали эффективный метод стереосопоставления на основе программируемых логических интегральных схем (ПЛИС) и оптимизированного сопоставления по сумме абсолютных разностей. 
Их алгоритм позволяет обрабатывать изображения со скоростью до 46 кадров в секунду при разрешении в $1280*1024$ пикселей. Ейнике и Эггерт \cite{multiblock} предложили алгоритм многоблочного сопоставления (Multi-Block-Matching), который комбинирует окна разного размера и формы вероятностным образом, что позволило значительно 
повысить качество выходных карт глубины. Позже их алгоритм был реализован на GPU, что позволило добиться производительности в 41 кадр в секунду при разрешении $2888*1920$ пикселей и 
большим диапазоне глубины \cite{multiblock_gpu}. 

\subsection{Сопоставление признаков}
Довольно распространёнными являются методы, основанные на поиске и сопоставлении характерных точек. За поиск этих точек отвечают методы 
SIFT \cite{sift}, SURF \cite{surf}, детектор Харриса и других. Различным способам поиска характерных точек посвящено немало статей. 

Суть методов, относящихся к этой группе, заключается в выделении на снимках характерных точек: углов и точек смены контраста. Далее 
для найденных точек считается дискриптор - вектор, являющийся численной характеристикой окрестности характерной точки. Таким образом,
установление соответствий сводится к сравнению численных величин этих векторов. 

Метод менее чувствителен к перекрытиям и слаботекстурированным областям, 
но, к сожалению, плотность точек, для которых возможно подсчитать глубину, получается относительно низкой. Результаты получаются быстро, но 
но поиск соответствий сильно усложняется при отличающихся ракурсах камер. Всё это привело к снижению интереса к группе методов.

Однако новые работы в этой области всё-таки продолжают публиковаться. Так был предложен метод на основе SIFT, использующий неиронные сети для 	
достижения более высокой производительности \cite{modsift} сопоставления признаков.   Работа Экстранда и других \cite{ekstrand2015high} же опирается на 
использование комбинации из сегментации изображения и обнаружения границ в качестве метрики поиска совпадений. Такая реализация работает быстро, но точность 
результатов по-прежнему остаётся невысокой, особенно в областях прерывистости. % discontinuity

% ГЛОБАЛЬНЫЕ
\subsection{Методы динамического программирования}
Методы, основанные на динамическом программировании, являются одними из самых часто используемых среди глобальных подходов.  
В случае с поиском совпадений на изображениях их применение означает использование конструкции DSI (disparity space image). Строится она следующим образом: 
выбираются $i$-ые строки левого и правого изображения (они должны быть ректифицированы), далее одна строка постепенно "сдвигается" относительно другой и
на каждом этапе одна из метрик из пункта \ref{BM} подсчитывается для совпадающей пары пикселей и записывается в DSI, который в итоге содержит в себе расхождения каждого пикселя 
с каждым для всех попарных рядов исходных изображений. Строки DSI, соответствующие заведомо невозможным значениям (например, расхождения больше максимально допустимых или пиксель на 
левом изображении находится левее соответствующего ему на правом) отбрасываются. Рисунок \ref{pic:DSI} \cite{DSI} иллюстрирует этот принцип. 
\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.04]{pics/DSI_rus.png}
		\caption{Процесс построения DSI. Строка левого изображения считается неподвижной, пока строка правого изображения смещается вдоль неё. Результат вычитания 
				 перекрывающихся пикселей складывается в очередную строку DSI. Сам DSI обрезается с учётом ограничений.	} 
		\label{pic:DSI} % название для ссылок внутри кода
	\end{center}
\end{figure}
Далее задача состоит в поиске оптимального пути от левого верхнего угла до правого нижнего на полученной матрице. Как уже упоминалось, динамическое программирование уменьшает 
вычислительную сложность задачи оптимизации  за счёт разбиения её на множество более простых и мелких задач. В данном случае это происходит за счёт того, что глобальная "стоимость" пути 
складывается рекурсивно из локальных, которые в свою очередь сводятся к стоимости движения по отдельным пикселям. Эта стоимость зависит от типа движения, который определяется величиной расхождения
в точке. Для каждого элемента матрицы возможны три типа "движения": горизонтальный (означающий совпадение), вертикальный и диагональный (оба соответствуют заслонённым областям). 
Это позволяет внести дополнительные ограничения и уменьшить число доступных путей. Тем не менее, количество путей, которые приходится подсчитывать, даже с этими ограничениями 
 остаётся довольно большим.

Данный алгоритм позволяет обрабатывать каждую строку изображения независимо и выполнять вычисления параллельно. Другими преимуществами является лучшее распознавание в слаботекстурированных 
областях по сравнению с локальными методами и возможность обработки заслонённых областей.   % % TODO: какой обработки? 
К недостаткам метода относят высокую вычислительную сложность (до $O(n^4)$, где n - количество пикселей в строке изображения) и возможность распространения локальной ошибки на последующие пиксели, что приводит к характерным горизонтальным полосам на картах глубины. 

Дополнительное ограничение, позволяющее уменьшить количество доступных путей, может дать использование заранее известных GCP (Ground Control Points). Эти точки, положение и 
соответствие которых можно определить заранее с большой точностью. Их использование позволяет снизить сложность поиска пути до 25\% от оригинальной задачи \cite{DSI}. 

Высокий интерес к этому методу привёл к появлению большого числа работ, пытающихся устранить недостатки метода и повысить его эффективность. Так была предложена модель \cite{symmetric}, которая 
опирается на ограничение видимости - предположение, что каждый заслонённый пиксель не имеет соответствия на втором изображении, а незаслонённый - обязательно имеет. В результате карты глубины не теряют информацию 
в местах, которые видит только одна камера, но при этом ухудшается детализация (рис. \ref{pic:symmetry}). 
\begin{figure}[H]
	\begin{center}
		\includegraphics[scale=0.3]{pics/symmetric_rus.png}
		\caption{ Сравнение карты глубины, полученной алгоритмом, с эталонным изображением. } 
		\label{pic:symmetry} % название для ссылок внутри кода
	\end{center}
\end{figure}

С точки зрения производительности в реальном времени глобальные методы, основанные на динамическом программировании, до сих пор показывают себя не лучшим образом из-за своей вычислительной сложности.
Однако всё-таки имеются успешные примеры подобных систем, реализованных на специальных вычислителях, способных воспользоваться возможностью параллелизации данных алгоритмов. Лараби и другие \cite{FPGA_DSI} 
смогли реализовать построение DSI с последующей его обработкой на ПЛИС и добиться производительности в 138 кадров в секунду для изображений $450*375$ пикселей. Реализацию на GPU осуществили Халлек и другие \cite{CUDA_DSI}. 
Они ввели новую метрику поиска соответствий, называемую ZSAD (сумма абсолютных разностей с нулевым средним), и построили с помощью неё DSI. Алгоритм достиг производит 29 кадров в секнуду в сравнительных тестах и 
показал большую точность в сравнении с другими предложенными авторами алгоритмами, работающими в реальном времени. 

\subsection{Алгоритм разреза графа}
Однако у предыдущего метода есть недостаток, выраженный в неспособности учитывать горизонтальную и вертикальную непрерывность поверхностей на снимке, так как столбцы/строки рассматриваются независимо. Эффективно исправить 
это в рамках динамического программирования пока не удалось. Альтернативный подход предлагает привести задачу к поиску максимального потока в графе. Для этого часто используется случайное поле Маркова (MRF) - неориентированный граф,
состоящий из независимых переменных, в качестве которых принято использовать пиксели левого изображения. Рёбра в таком случае отражают зависимости между связываемыми ими пикселями. Таким образом, граф выглядит как сетка, в которой 
переменные (пиксели) зависят только от своих соседей (локальное свойство Маркова), и задачу стереозрения можно переформулировать и решать в терминах MRF.  
Тогда каждая переменная может принимать значения, соответствующие диапазону расхождений, а каждое рёбро - пропускную способность, зависящую от значений соединяемых им пикселей.  
Для решения задачи стереозрения могут использоваться известные алгоритмы минимизации энергии на случайных полях Маркова. Например, карта расхождений по этому методу находится как плоскость, формирующая разрез представленного графа на две части с минимальной 
суммарной стоимостью входящих в него рёбер. Особенности построения и математическая формализация этих структур приведены в работах \cite{graphcut,gc_ocl}. 

Очевидно, что классические методы, основанные на алгоритме разреза графа, требуют даже больше вычислительных операций, чем динамическое программирование. Были предприняты попытки ускорить метод \cite{fast_gc,graphcut}, 
которые приблизили его по эффективности к динамическому программированию. Помимо вычислительных ресурсов алгоритм так же довольно требователен к памяти (до  150 раз больше объёма одного изображения), что осложняет его использование 
с большими изображениями, но и эта проблема решается применением более эффективных структур данных \cite{effic_gc}. Однако такая модификация алгоритма показывает результаты в среднем лишь на 1.6\%  лучше, чем ДП. Он также был реализован 
на ПЛИС, что позволило добиться существенного увеличения скорости относительно классического метода на ЦПУ, но пока с сильным дисбалансом по использованию ресурсов вычислителя. \cite{fpga_gc}. В 2018 году была предложена версия алгоритма, 
достигшая производительности в 6 кадров в секунду \cite{jetson_gc}. При этом использовался вычислитель NVIDIA Jetson TX2, часто применяемый в мобильной робототехнике. 
Тем не менее, современные алгоритмы разреза графа являются одними из самых медленных, и самых  точных, алгоритмов стереозрения. 

\subsection{Алгоритмы на нейронных сетях}

Рост доступного объёма памяти в графических ускорителях и популярности алгоритмов, построенных с использованием неиросетей, а также возникновение качественных датасетов позволили реализовать надёжные и быстрые способы 
получения карт глубины, опирающиеся на глубокое обучение \cite{neural_review}. В настоящий момент существенная доля самых точных алгоритмов в популярном бенчмарке Middlebury Stereo Evaluation \cite{stereo_bench} представлена именно
подобными решениями. 

При этом большинство алгоритмов реализуют один из двух основных подходов. Первый заключается в том, что свёрточные нейронные сети используются для выполнения лишь одного этапа из более комплексного алгоритма стереозрения, 
например, только поиск соответствий в эпиполярной линии \cite{cnn_match}. Более современные работы экспериментируют с архитектурой сети, добиваясь, например, подсчёта стоимости сопоставления для каждого возможного расхождения аналогично
глобальным методам \cite{cnn_improv}.  К этому же подходу можно отнести использование неиронных сетей для постобработки карт глубины \cite{cnn_post1}.  
Второй подход подразумевает решение всей задачи получения карты глубины с помощью глубокого обучения без необходимости в пост-обработке. Обучение и дизайн сети при таком подходе обычно проще, а алгоритмы получаются более быстрыми (относительно первого подхода).
 DispNetC \cite{cnn_ete1}, например, использует подход, который ранее применялся для решения задачи вычисления оптического потока (тоже заключается в поиске соответсвий, но обычно по двум последовательным снимкам одной камеры), 
 которую можно считать более общей, так как расхождения могут быть найдены по любому направлению, а не только в рамках одного ряда. Наложение дополнительных ограничений позволило получить точный и относительно быстрый метод подсчёта карт глубины, 
способный на производительность в реальном времени при использовании мощного графического ускорителя. GC-Net \cite{gc_net} впервые использовала модуль 3D-свёртки, который позже стал основой для целого класса объёмных методов. Текущий 
лидер бенчмарка и датасета KITTI 2015 \cite{kitti} LEAStereo \cite{top_net} тоже является объёмным.

Таким образом, современные алгоритмы на глубоком обучении позволяют оценивать глубину по паре изображений с высокой точностью и производительностью в реальном времени для некоторых алгоритмов. Тем не менее их использование затруднено 
необходимостью использования современных графических ускорителей с большим энергопотреблением. 


\section{Выводы}
\label{conclusion}

Задача 3D-реконструкции остаётся активным полем работы для исследователей, занимающихся техническим зрением. В этой работе были представлены основные алгоритмы, применяемые на данный момент. По изученной литературе видно, что за последние два десятилетия  
фокус сместился с фундаментальных основ стереозрения к более частным исследованиям, направленным на повышение качества и скорости получения результатов, а также проблемным областям вроде обработки заслонений и слаботекстурированных регионов. Использование нейросетей 
для данной задачи также привело к появлению нового весьма перспективного класса алгоритмов, который, вероятно, будет становиться только популярнее. В настоящий момент присутствует запрос на эффективные методы стереозрения, способные работать в реальном времени и на 
мобильных платформах, удовлетворению этого запроса, вероятно, и будут посвящены дальнейшие исследования на эту тему. 



\newpage
\bibliographystyle{./config/splncs04}
\bibliography{refs}

\end{document}
