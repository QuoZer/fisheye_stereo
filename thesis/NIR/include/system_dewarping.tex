\subsection{Алгоритм устранения искажений}
\label{dewarping}

Распространённые библиотеки машинного зрения предлагают готовые к использованию классы и функции, 
позволяющие устранять искажения fisheye-объективов. Например, в составе MATLAB Computer Vision Toolbox присутствует 
инструмент Camera Calibrator, позволяющий в несколько простых этапов  произвести оценку внутренних и внешних параметров
 камеры. Пользователю надо лишь  загрузить набор изображений, содержащих в себе калибровочный  узор, выбрать модель камеры 
(стандартная или сверхширокоугольная) и запустить автоматический процесс калибровки. По завершению процесса программа 
показывает точность калибровки и может отображать скорректированные изображения. Результаты можно экспортировать в рабочую 
область MATLAB для дальнейшей обработки изображений. Интерфейс программы представлен на рисунке \ref{pic:calibrator}. 

 \addimghere{calibrator}{0.9}{Интерфейс MATLAB Camera Calibrator}{pic:calibrator} 

Однако на практике этот и другие подобные методы весьма ограничены. И MATLAB, и OpenCV устраняют искажения  строго 
в центральной части изображения, не позволяя выбирать  желаемое направление обзора.  Это ограничивает применимость 
существующих инструментов только для копланарного расположения  камер. Поэтому для реализации предлагаемой системы 
стереозрения  был разработан собственный алгоритм устранения искажений. Схема геометрического принципа, лежащего в
 основе этого алгоритма, представлена на рисунке \ref{pic:sweeping}.

\addimghere{projection_sweeping}{0.7}{Схема принципа устранения искажений}{pic:sweeping} 

Цель алгоритма - найти, куда на выходном изображении  с  перспективной проекцией проектируются все пиксели из 
выбранного участка входного сверхширокоугольного изображения. Сделать это можно, выполнив обратное преобразование 
(\ref{eqn:scaramuzza}) для каждого пикселя fisheye-снимка и затем прямое преобразование модели камеры-обскуры 
для получения результирующей проекции. Однако такой подход приведёт к возникновению дефектов из-за несовпадения % TODO:  скорее всего наврал. Надо проконсультироваться 
частот дискретизации двух изображений.  Поэтому вместо этого алгоритм сначала выполняет обратное преобразование  для 
каждого пикселя итогового изображения $\nu$, находя таким образом соответствующую ему точку в системе координат 
камеры $({X_c, Y_c, Z_c})$ 
\begin{equation}
    \label{eq:uv_to_xyz}
    \left[\begin{matrix}x_c\\y_c\\z_c\\\end{matrix}\right] = \left[\begin{matrix} {u*z_c}/f \\  {v*z_c}/f \\ z_c \\\end{matrix}\right],
\end{equation} 
где  $f$ - фокусное расстояние. 

Набор таких точек формирует прямоугольную область $\nu_p$ с центром в точке $O_p$ и является плоскостью изображения
 виртуальной камеры-обскуры  с оптической осью $Z_p$. Поворот точек, входящих в $\nu_p$, с помощью матрицы вращения 
 $\bm{R}$ образует $\nu'_p$ и позволяет таким образом задать направление обзора и ориентацию виртуальной камеры. 
\begin{equation}
    \label{eq:sweeped}
    \left[\begin{matrix}x'_p\\y'_p\\z'_p\\\end{matrix}\right] = \left[\begin{matrix}x_p\\y_p\\z_p\\\end{matrix}\right] \bm{R}.
\end{equation}  
\begin{equation}
    \label{eq:R}
    \bm{R} = \left[\begin{matrix}\cos{\alpha}&-\sin{\alpha}&0\\\sin{\alpha}&\cos{\alpha}&0\\0&0&1\\\end{matrix}\right]\left[\begin{matrix}1&0&0\\0&\cos{\beta}&\sin{\beta}\\0&-\sin{\beta}&\cos{\beta}\\\end{matrix}\right]\left[\begin{matrix}\cos{\gamma}&0&-\sin{\gamma}\\0&1&0\\\sin{\gamma}&0&\cos{\gamma}\\\end{matrix}\right],
\end{equation} 
где координаты с индексом $p$ -  координаты точек соответствующей плоскости $\nu$ в системе камеры, $\alpha, \beta, \gamma$ - углы Эйлера, управляющие  ориентацией виртуальной камеры-обскуры. % TODO: точно ли Эйлера? 

Тогда обратная fisheye-проекция точек из $\nu'_p$ позволяют получить область $\nu_i$ исходного изображения с искомыми пикселями. 
Таким образом, зная как  проектируется каждая точка из $\nu$ в $\nu_i$, возможно  обратно перенести информацию о цвете на итоговое изображение.  

Так как в используемой модели искажения хода луча считаются центрально симметричными и зависят только от его удаления от центра изображения, 
рассмотрим ход падающего луча в координатах $(Z_c, \rho)$, изображённый на рисунке \ref{pic:scara_graph}.

\addimghere{scara_graph}{0.5}{Нахождение обратной проекции для используемой модели}{pic:scara_graph} 

Для модели (\ref{eqn:scaramuzza}) обратная проекция записывается как 
\begin{equation}
    \label{eq:back_scara}
    \left[\begin{matrix}u_i\\v_i\\\end{matrix}\right] = \left[\begin{matrix} \frac{x_c}{\lambda}  \\  \frac{y_c}{\lambda} \\\end{matrix}\right],
\end{equation}  
где $\lambda = \rho_c / \rho_i$ - масштабный коэффициент. 

Для нахождения $\rho_i$ был применён метод последовательных приближений. Блок-схема алгоритма, реализующего обратную проекцию, изображена  
на рисунке  \ref{pic:newton_scheme}.  Сравнение изображений инструмента  калибровки и описанного алгоритма для центральной области изображения
представлено на рисунке \ref{pic:central_pics}. 

\addimghere{remapped_images}{0.8}{Изображения, скорректированные алгоритмом (слева) и MATLAB (справо)}{pic:central_pics} 

Очевидно, весь процесс преобразования $\nu \rightarrow \nu_i$ требует выполнения существенного количества математических операций, что 
негативно сказывается на скорости, с которой алгоритм может обрабатывать изображения в реальном времени. Однако при неизменных параметрах 
модели алгоритм достаточно выполнить лишь один раз, записав результат в таблицу поиска - структуру данных, которая позволяет дальнейшие 
преобразования проводить по уже известным соотношениям между пикселями. Это позволяет применять алгоритм для устранения 
искажений в реальном времени.


\addimghere{flowchart}{0.5}{Блок-схема алгоритма обратной проекции}{pic:newton_scheme} 